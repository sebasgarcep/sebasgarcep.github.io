[ { "title": "Understanding the Kalman Filter", "url": "/posts/kalman-filter-tutorial/", "categories": "", "tags": "Mathematics, Data Assimilation, Kalman Filter, Rust", "date": "2022-01-08 00:00:00 -0500", "snippet": "Society depends on having correct estimates of what may happen. Whether it is the climate or traffic, humans have built models of reality which help us make better decisions. But any model is bound to have a margin of error with respect to reality, and as time passes the model will diverge from reality. To prevent this we would like to integrate observations from reality into the model from time to time. But we run into the problem that observations of reality are often incomplete or have a margin of error themselves. The Kalman Filter is a method of combining both sources of information (model state with observations) in a way that reduces the margin of error in the output.This post will introduce the Kalman Filter, prove it and implement it in Rust. If you are uncomfortable with math, this is the moment to back out or jump to the implementation part as the presentation will be math-heavy.Introducing the Kalman FilterAssume that a variable $x \\in \\mathbb{R}^m$ models the state of a system that evolves over time. At a time step $t$ we have an estimation of the current state we will call $x_{\\text{esti}}$. We have a linear map $F$ that helps us predict the next state of the system $x_{\\text{pred}} := F x_{\\text{esti}}$, and we have a measurement of the next state of the system $z \\in \\mathbb{R}^n$. Notice that the measurement uses another letter. This is to denote the fact that the oberservation and the model state are not necessarily on the same space, e.g. the observation may have lower dimensionality. Assume there is a linear function $H$ that maps from model space to observation space.Now we need to understand and model all our sources of uncertainty. To simplify things we will assume all errors are normally distributed with mean zero. This is not always true, but it is a robust enough assumption that even when it doesn’t hold the Kalman Filter still provides good enough results. Firstly, the model state will always have a discrepancy with the real system. Let $P_{\\text{esti}}$ be the covariance matrix of the model state noise. Secondly, physical observations have a degree of uncertainty as no real-life tool is perfect. Let $R$ describe the covariance matrix of the observation noise. Finally, our prediction function will rarely be a complete model of reality and this discrepancy will introduce errors into the calculations. Let $Q$ be the covariance matrix of process noise.With all the setup out of the way we can introduce the Kalman Filter Equations. First consider the equations that propagate the model state and covariance\\[\\begin{align*}x_{\\text{pred}} &amp;amp;= F x_{\\text{esti}} \\\\P_{\\text{pred}} &amp;amp;= F P_{\\text{esti}} F^T + Q \\\\\\end{align*}\\]Now, let the Kalman Gain $K$ be\\[K = P_{\\text{pred}} H^T (H P_{\\text{pred}} H^T + R)^{-1}\\]Then the correction in model state $x_{\\text{corr}}$ and covariance $P_{\\text{corr}}$ obtained by applying the Kalman Filter is given by the following set of formulas\\[\\begin{align*}x_{\\text{corr}} &amp;amp;= x_{\\text{pred}} + K (z - H x_{\\text{pred}}) \\\\P_{\\text{corr}} &amp;amp;= (I - K H) P_{\\text{pred}} (I - K H)^T + K R K^T\\end{align*}\\]Which we will formally prove in the next section.Deriving the Kalman FilterFrom the setup we have that $x_{\\text{pred}}$ is sampled from $\\mathcal{N}(x_{\\text{real}}, P_{\\text{pred}})$ and $z$ is sampled from $\\mathcal{N}(H x_{\\text{real}}, R)$. We cannot use these expressions to find $x_{\\text{real}}$ but we can find a good approximation $x_{\\text{corr}}$. Consider the following expression given by maximum likelihood estimation:\\[\\frac{1}{\\sqrt{(2 \\pi)^m |P_{\\text{pred}}|}} e^{-\\frac{1}{2} (x_{\\text{pred}} - x_{\\text{corr}})^T P_{\\text{pred}}^{-1} (x_{\\text{pred}} - x_{\\text{corr}})} \\cdot \\frac{1}{\\sqrt{(2 \\pi)^n |R|}} e^{-\\frac{1}{2} (z - H x_{\\text{corr}})^T R^{-1} (z - H x_{\\text{corr}})}\\]which can be rewritten to\\[\\frac{1}{\\sqrt{(2 \\pi)^{m + n} |P_{\\text{pred}}| |R|}} e^{-\\frac{1}{2} ((x_{\\text{pred}} - x_{\\text{corr}})^T P_{\\text{pred}}^{-1} (x_{\\text{pred}} - x_{\\text{corr}}) + (z - H x_{\\text{corr}})^T R^{-1} (z - H x_{\\text{corr}}))}\\]We would like to find the value of $x_{\\text{corr}}$ that optimizes the previous expression. For that we need to optimize the argument of the exponential function:\\[(x_{\\text{pred}} - x_{\\text{corr}})^T P_{\\text{pred}}^{-1} (x_{\\text{pred}} - x_{\\text{corr}}) + (z - H x_{\\text{corr}})^T R^{-1} (z - H x_{\\text{corr}})\\]To find the optimal value of $x_{\\text{corr}}$ we derive this expression with respect to it and equate the result to zero. We are left with the following equation\\[\\begin{align*}&amp;amp;0 = -2 (x_{\\text{pred}} - x_{\\text{corr}})^T P_{\\text{pred}}^{-1} - 2 (z - H x_{\\text{corr}})^T R^{-1} H \\\\&amp;amp;\\Rightarrow 0 = (x_{\\text{pred}} - x_{\\text{corr}})^T P_{\\text{pred}}^{-1} + (z - H x_{\\text{corr}})^T R^{-1} H \\\\&amp;amp;\\Rightarrow 0 = P_{\\text{pred}}^{-T} (x_{\\text{pred}} - x_{\\text{corr}}) + H^T R^{-T} (z - H x_{\\text{corr}}) \\\\&amp;amp;\\Rightarrow 0 = P_{\\text{pred}}^{-1} (x_{\\text{pred}} - x_{\\text{corr}}) + H^T R^{-1} (z - H x_{\\text{corr}}) \\\\&amp;amp;\\Rightarrow (P_{\\text{pred}}^{-1} + H^T R^{-1} H) x_{\\text{corr}} = P_{\\text{pred}}^{-1} x_{\\text{pred}} + H^T R^{-1} z \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = (P_{\\text{pred}}^{-1} + H^T R^{-1} H)^{-1} (P_{\\text{pred}}^{-1} x_{\\text{pred}} + H^T R^{-1} z) \\\\\\end{align*}\\]An application of the Woodbury matrix identity shows that\\[(P_{\\text{pred}}^{-1} + H^T R^{-1} H)^{-1} = P_{\\text{pred}} - P_{\\text{pred}} H^T (H P_{\\text{pred}} H^T + R)^{-1} H P_{\\text{pred}} = P_{\\text{pred}} - K H P_{\\text{pred}}\\]where the last part follows from the definition of the Kalman Gain $K$. Therefore\\[\\begin{align*}&amp;amp;x_{\\text{corr}} = (P_{\\text{pred}}^{-1} + H^T R^{-1} H)^{-1} (P_{\\text{pred}}^{-1} x_{\\text{pred}} + H^T R^{-1} z) \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = (P_{\\text{pred}} - K H P_{\\text{pred}}) \\cdot (P_{\\text{pred}}^{-1} x_{\\text{pred}} + H^T R^{-1} z) \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = x_{\\text{pred}} - K H x_{\\text{pred}} + (P_{\\text{pred}} H^T R^{-1} - K H P_{\\text{pred}} H^T R^{-1}) z \\\\\\end{align*}\\]But\\[\\begin{align*}&amp;amp;K = P_{\\text{pred}} H^T (H P_{\\text{pred}} H^T + R)^{-1} \\\\&amp;amp;\\Rightarrow K (H P_{\\text{pred}} H^T + R) = P_{\\text{pred}} H^T \\\\&amp;amp;\\Rightarrow K H P_{\\text{pred}} H^T = P_{\\text{pred}} H^T - K R \\\\\\end{align*}\\]Which applied to the previous formula yields\\[\\begin{align*}&amp;amp;x_{\\text{corr}} = x_{\\text{pred}} - K H x_{\\text{pred}} + (P_{\\text{pred}} H^T R^{-1} - K H P_{\\text{pred}} H^T R^{-1}) z \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = x_{\\text{pred}} - K H x_{\\text{pred}} + (P_{\\text{pred}} H^T R^{-1} - (P_{\\text{pred}} H^T - K R) R^{-1}) z \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = x_{\\text{pred}} - K H x_{\\text{pred}} + (P_{\\text{pred}} H^T R^{-1} - P_{\\text{pred}} H^T R^{-1} + K) z \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = x_{\\text{pred}} - K H x_{\\text{pred}} + K z \\\\&amp;amp;\\Rightarrow x_{\\text{corr}} = x_{\\text{pred}} + K (z - H x_{\\text{pred}}) \\\\\\end{align*}\\]which proves the first of the update formulas. The second results from noticing that the previous formula can be rewritten as\\[x_{\\text{corr}} = (I - K H) x_{\\text{pred}} + K z\\]and therefore the update formula for the covariance is\\(P_{\\text{corr}} = (I - K H) P_{\\text{pred}} (I - K H)^T + K R K^T\\)dwhich finalizes the proof.Applying the Kalman FilterSuppose we have a signal-repeating satellite orbiting around the Earth. Thanks to Kepler’s laws of planetary motion we know that this orbit is elliptical, with the Earth at one of its foci. Any ellipse can be described by its semimajor axis $a$ (half the length of its largest diameter) and its semiminor axis $b$ (half the length of its smallest diameter). The focal distance (distance from the center of the ellipse to either of the foci) can be found with the formula $c = \\sqrt{a^2 - b^2}$. The orbit of the satellite and the position of the Earth relative to it will look something like this plot:Because we have a signal-repeating satellite orbiting the earth, we can measure how far away from the Earth it is by bouncing a signal and measuring how long it takes to come back (the signal will travel at the speed of light). We can use this tool to measure the periapsis (closest distance to Earth) and apoapsis of the orbit (furthest away from Earth). Clearly\\[\\begin{align*}\\text{periapsis} &amp;amp;= a - c \\\\\\text{apoapsis} &amp;amp;= a + c \\\\\\end{align*}\\]and we can solve this system of equations to obtain the values of $a$ and $c$. The value of $b$ can be obtained by using the formula $c = \\sqrt{a^2 - b^2}$. With this information we can model the orbit of the satellite by solving Kepler’s equation.In real life we are going to face the problem that the measurement of the periapsis and apoapsis will be noisy, which means we won’t have a perfect approximation of the orbit of the satellite. We also cannot constantly measure the position of the satellite in space (using the distance and the angle relative to Earth) as each measurement will have some amount of noise to it, and the farther away the satellite is, the noisier the measurements get. We can observe this behaviour in the following graphUsing the Kalman Filter we can combine both sources of information though (model state and measurements) to obtain better results.We will model the motion of the satellite using a constant position model, i.e. we will set $F$ to be the identity map. This may seem like an inappropiate choice, but later on we will show that it gives good enough results for a short exercise like this. We will set $Q = \\sigma I$, where sigma is the uncertainty in distance measurements.The satellite gives us a distance-angle reading which satify the hypothesis that the error in our observations should Gaussian with zero mean. We will not use this observation directly in the Kalman Filter. The reason is that two angles can be physically very close and numerically very far away, e.g. doing two and a half turns around the circle gives the same angle as doing just half a turn, but numerically they are distinct by $4 \\pi$ radians. This will give us problems later on when we calculate $z - H x_{\\text{pred}}$ in the Kalman Filter equations. Instead we will map this observation into cartesian coordinates. Therefore the observation map $H$ is the identity map.Notice that this operation on the observation transforms the distribution of our errors. Specifically, the further away we get from the foci, the larger the errors get, as we can see in the above graph. We can get around this by scaling $R$ by the distance squared between the satellite and the Earth.With all this setup done we can now implement the Kalman Filter and our model.Rust ImplementationLet’s start by implementing the Kalman Filter as a Trait:use crate::types::{ArrayMatrix, ArrayVector};use nalgebra::DMatrix;pub trait KalmanFilterModel { fn predict_mat(&amp;amp;self, x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix; fn predict_cov(&amp;amp;self, x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix; fn observe_mat(&amp;amp;self, x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix; fn observe_cov(&amp;amp;self, x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix; fn predict(&amp;amp;self, x: &amp;amp;ArrayVector, p_inpt: &amp;amp;ArrayMatrix) -&amp;gt; (ArrayVector, ArrayMatrix) { let f = self.predict_mat(x); let q = self.predict_cov(x); let x_pred = &amp;amp;f * x; let p_pred = &amp;amp;f * p_inpt * &amp;amp;f.transpose() + &amp;amp;q; (x_pred, p_pred) } fn update(&amp;amp;self, x: &amp;amp;ArrayVector, p: &amp;amp;ArrayMatrix, z: &amp;amp;ArrayVector) -&amp;gt; (ArrayVector, ArrayMatrix) { let model_size = x.len(); let i: ArrayMatrix = DMatrix::identity(model_size, model_size); let h = self.observe_mat(x); let r = self.observe_cov(x); let k = p * &amp;amp;h.transpose() * (&amp;amp;h * p * &amp;amp;h.transpose() + &amp;amp;r).try_inverse().unwrap(); let x_hat = x + &amp;amp;k * (z - &amp;amp;h * x); let p_hat = (&amp;amp;i - &amp;amp;k * &amp;amp;h) * p * (&amp;amp;i - &amp;amp;k * &amp;amp;h).transpose() + &amp;amp;k * &amp;amp;r * &amp;amp;k.transpose(); (x_hat, p_hat) }}This allows each model to provide its own implementation of the $F$ (predict_mat), $Q$ (predict_cov), $H$ (observe_mat) and $R$ (observe_cov) matrices while allowing us to define a default behaviour for the predict and update equations.Our model, which we named ConstantOrbitModel has the following implementation:use crate::filters::KalmanFilterModel;use crate::types::{ArrayMatrix, ArrayVector};use nalgebra::{dmatrix, DMatrix};pub struct ConstantOrbitModel { pub position_noise: f64, pub e_meas: ArrayVector,}impl ConstantOrbitModel { pub fn new( position_noise: f64, e_meas: ArrayVector, ) -&amp;gt; Self { ConstantOrbitModel { position_noise, e_meas, } }}impl KalmanFilterModel for ConstantOrbitModel { fn predict_mat(&amp;amp;self, _x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix { DMatrix::identity(2, 2) } fn predict_cov(&amp;amp;self, _x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix { dmatrix![ self.position_noise, 0.0; 0.0, self.position_noise; ] } fn observe_mat(&amp;amp;self, _x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix { DMatrix::identity(2, 2) } fn observe_cov(&amp;amp;self, x: &amp;amp;ArrayVector) -&amp;gt; ArrayMatrix { dmatrix![ self.position_noise * (x - &amp;amp;self.e_meas).norm_squared(), 0.0; 0.0, self.position_noise * (x - &amp;amp;self.e_meas).norm_squared(); ] }}which does the same things we discussed in the previous section.Finally we implement the orbit simulation and the functions that output the results to the SVG graphs you are seeing. I will not discuss those parts here, as they are outside the scope of the tutorial, but you can take a look at the complete source code here.ConclusionAfter implementing the Kalman Filter we obtain the following resultsThe green line shows the measurements over time, and the yellow line shows the orbit predicted by our Kalman Filter model. As we can see it is far more stable and a better approximation of the real orbit than the measurements themselves. In fact the relative error rate over time is far smaller with the Kalman Filter corrections than without them:And this is with a constant position model! With better models or numerical stabilization procedures we are sure to obtain even better results. This is the power of the Kalman Filter.I hope that this article gave you a better understanding of the Kalman Filter and that you may be able to apply it to your projects soon.References Source code for the tutorial http://web.mit.edu/kirtley/kirtley/binlustuff/literature/control/Kalman%20filter.pdf https://en.wikipedia.org/wiki/Kalman_filter https://en.wikipedia.org/wiki/Woodbury_matrix_identity https://en.wikipedia.org/wiki/Kepler’s_laws_of_planetary_motion https://en.wikipedia.org/wiki/Kepler’s_laws_of_planetary_motion#Position_as_a_function_of_time https://en.wikipedia.org/wiki/Kepler%27s_equation#Numerical_approximation_of_inverse_problem" }, { "title": "Writing a React Native App using Expo and Google Social Login", "url": "/posts/social-login-app/", "categories": "", "tags": "Software Engineering, React Native, Expo, Social Login, Express, Passport, Google", "date": "2022-01-03 00:00:00 -0500", "snippet": "This will be a brief tutorial on how to write a React Native application, using Expo, that implements Social Authentication, using Google. We will use ExpressJS and Passport in the backend to manage the authentication process. We will assume that the reader is already comfortable with React Native and Express and that they are also proficient with Typescript.Designing the ApplicationOur client wants a mobile application that allows people to login using any of the major social networks and which feeds authenticated users a random message. Because we will set up everything to be easily extensible we will only implement one of the social authentication providers (Google), as that is enough for an exercise. The application must therefore consist of two screens: Login screen: where the login workflow occurs. Home screen: where the user can obtain the random message.The backend must implement at least the following two endpoints: POST /api/v1/auth/google: If the login attempt was succesful the mobile device will receive a Google ID token. That token will be sent to this endpoint to trade for a valid session token. GET /api/v1/message: where authenticated users can obtain a random message.Notice that we do not use the Google ID token as the session token for several reasons. First, it will require us to perform an authentication check on each API call to the Google backend, which will slow down the server response time. Second, Google ID tokens are usually short-lived, so users will be forced to login again after a short amount of time. Third, because we want multiple login providers, trading provider-specific ID tokens for application session tokens simplifies authentication workflows on restricted endpoints.Creating the Login ScreenStart by scaffolding a React Native/Expo App with Typescript. You are going to need to run the following command in order to install the necessary dependencies for social authentication.expo install expo-auth-session expo-web-browser expo-randomI also like adding React Native Paper for some nice GUI elements, React Navigation for the navigation provider and Redux to use as a global state store.Once you are ready, create a LoginScreen component as follows:import { StackScreenProps } from &#39;@react-navigation/stack&#39;;import * as React from &#39;react&#39;;import { StyleSheet } from &#39;react-native&#39;;import { Button, Title } from &#39;react-native-paper&#39;;import Container from &#39;../components/Container&#39;;import { RootStackParamList } from &#39;../types&#39;;import useGoogleLogin from &#39;../hooks/useGoogleLogin&#39;;import { useSession } from &#39;../store/selectors/session&#39;; export default function LoginScreen({ navigation,}: StackScreenProps&amp;lt;RootStackParamList, &#39;Login&#39;&amp;gt;) { const session = useSession(); const [loadingGoogleLogin, promptGoogleLogin] = useGoogleLogin(); React.useEffect(() =&amp;gt; { if (session) { navigation.replace(&#39;Home&#39;); } }, [session]); return ( &amp;lt;Container style={styles.container}&amp;gt; &amp;lt;Title style={styles.title}&amp;gt;Social Login App&amp;lt;/Title&amp;gt; &amp;lt;Button disabled={loadingGoogleLogin} icon=&quot;google&quot; mode=&quot;contained&quot; onPress={promptGoogleLogin} &amp;gt; Login with Google &amp;lt;/Button&amp;gt; &amp;lt;/Container&amp;gt; );}const styles = StyleSheet.create({ container: { padding: 20, justifyContent: &#39;center&#39;, alignItems: &#39;center&#39;, }, title: { fontSize: 28, marginBottom: 60, },});The important aspects of this screen are: useSession hook: trivial state selector that checks that the session exists in the data store and hasn’t expired. Returns the session if this holds, otherwise it returns null. useGoogleLogin hook: Handles the Google Login logic on the mobile side. Returns two values to be used by the frontend. loadingGoogleLogin which is a boolean value that tells the frontend whether Google Login is running in the background, and promptGoogleLogin which triggers the authentication workflow. React.useEffect call: If it observes that the session changes to a non-null value, then it moves the user to the Home screen. Notice that if the application is closed and opened again this piece of code will redirect the user to the Home screen if we persist the data store.Let’s dive deeper into the useGoogleLogin implementation:import * as Google from &#39;expo-auth-session/providers/google&#39;;import * as WebBrowser from &#39;expo-web-browser&#39;;import * as React from &#39;react&#39;;import { GOOGLE_CLIENT_ID } from &#39;../constants/environment&#39;;import { loginWithGoogle } from &#39;../services/server&#39;;import { useLogin } from &#39;../store/actions/session&#39;;WebBrowser.maybeCompleteAuthSession();export default function useGoogleLogin(): [boolean, () =&amp;gt; any] { const login = useLogin(); const [request, response, promptAsync] = Google.useIdTokenAuthRequest({ clientId: GOOGLE_CLIENT_ID, }); React.useEffect(() =&amp;gt; { if (response?.type === &quot;success&quot;) { const { id_token: idToken } = response.params; loginWithGoogle(idToken) .then(session =&amp;gt; login(session)); } }, [response]); const loading = !request; const prompt = () =&amp;gt; { promptAsync(); }; return [loading, prompt];}First notice tha usage of WebBrowser.maybeCompleteAuthSession(). This is required in certain environments for the authentication procedure to work. In other environments it is a no-op, so it can be added safely.First notice the Google.useIdTokenAuthRequest hook. It takes a configuration object as input, with the only required key being the Google Client ID. To obtain a Google Client ID you need to configure a project in GCP and create a OAuth 2.0 Client ID. Further information in how to set up Google Client IDs can be found here. Notice that each environment (Expo Go, Android Standalone, iOS Standalone) requires a different key. Make sure to set up your application to use different keys for each environment.The Google.useIdTokenAuthRequest hook returns three values: the request object if no workflow is in progress, or null otherwise. the response object after an auth workflow has succeeded or failed. the promptAsync function that triggers the auth workflow.We need to track the request to determine whether a workflow is in progress. We also have to track the response object to trigger the login action if the workflow succeeds. Finally, we need to return the prompt function to allow consumers of useGoogleLogin to trigger the authentication procedure.Once we have a response with a success type, we know we have obtained a Google ID token. Now we can use the loginWithGoogle function to trade it for a valid session token. Once we have a session we have to update the global data store with it. The useLogin hook returns a login function that takes a session as input and updates the global data store using said value.With all of this we have a working Login screen. Now let’s proceed to the server-side implementation.Configuring Passport for Google AuthFirst let’s set up a Node Project with Typescript enabled. Once we have that we can install the following dependencies:yarn add debug express morgan cookie-parser passport passport-google-id-token jsonwebtoken passport-jwtWe also need to add the following dependencies with the dev flag on so that our typescript compiler will work properly.yarn add -D @types/cookie-parser @types/debug @types/express @types/morgan @types/node @types/passport @types/passport-jwtSadly, the passport-google-id-token has no official Typescript bindings at the time of this writing. I had to write my own and put it in the index.d.ts file at the root of the project. What I ended up using was:declare module &quot;passport-google-id-token&quot; { import { Strategy } from &quot;passport&quot;; type GoogleTokenStrategyOptions = { clientID?: string, }; type DoneFunction = (err: Error | null, user?: any) =&amp;gt; any; type GoogleTokenStrategyCallback = (parsedToken: any, googleId: string, done: DoneFunction) =&amp;gt; any; export default class GoogleTokenStrategy extends Strategy { constructor(options: GoogleTokenStrategyOptions, callback: GoogleTokenStrategyCallback); }}I’ve also added Prisma to serve as an ORM, Postgres to act as the database and Axios to perform API calls to external services, but you can add whichever services you prefer.Now, let’s start by writing a function in src/app/middleware.ts that adds useful middleware to the Express Application:import cookieParser from &#39;cookie-parser&#39;;import express, { Express } from &quot;express&quot;;import logger from &#39;morgan&#39;;import path from &#39;path&#39;;export default function(app: Express) { app.use(logger(&#39;dev&#39;)); app.use(express.json()); app.use(express.urlencoded({ extended: false })); app.use(cookieParser()); app.use(express.static(path.join(__dirname, &#39;public&#39;)));}Now we have to configure Passport. Consider the following function in the file src/app/passport.ts:import createDebug from &#39;debug&#39;;import { Express } from &#39;express&#39;;import passport, { Strategy } from &#39;passport&#39;;import GoogleTokenStrategy from &#39;passport-google-id-token&#39;;import { Strategy as JWTStrategy, ExtractJwt } from &#39;passport-jwt&#39;;import * as User from &#39;../controllers/user&#39;;import { Session } from &#39;../types&#39;;import { GOOGLE_CLIENT_ID, JWT_SECRET } from &#39;../values&#39;;const debug = createDebug(&#39;social-login-server:app:passport&#39;);function getGoogleTokenStrategy(clientID: String): Strategy { return new GoogleTokenStrategy({ clientID, }, (parsedToken, googleId, done) =&amp;gt; { debug(`Parsed Token: ${JSON.stringify(parsedToken, null, 4)}`); debug(`Google ID: ${googleId}`); User.getOrCreateUserByGoogleId(googleId) .then(user =&amp;gt; done(null, user)) .catch(err =&amp;gt; done(err)); });}function getJWTStrategy(): Strategy { return new JWTStrategy({ jwtFromRequest: ExtractJwt.fromAuthHeaderAsBearerToken(), secretOrKey: JWT_SECRET, }, (payload: Session, done) =&amp;gt; { debug(`JWT Payload: ${JSON.stringify(payload, null, 4)}`); if (Date.now() &amp;gt;= payload.expiresAt) { done(new Error()); return; } done(null, payload); });}export default function(app: Express) { app.use(passport.initialize()); passport.serializeUser((user, done) =&amp;gt; { done(null, user); }); passport.use(&#39;google-id-token&#39;, getGoogleTokenStrategy(GOOGLE_CLIENT_ID)); passport.use(&#39;jwt&#39;, getJWTStrategy());}First we have to initialize passport with the call app.use(passport.initialize()). After that we have to add a serializeUser function to passport. Whenever we login, it transforms the incoming login credentials into a session. We won’t need to do anything here as we will be using the Authentication header, and not cookies. Therefore an identity function will suffice. Finally, we add two strategies to passport: GoogleTokenStrategy: Extracts the Google ID token from the body of the request. Specifically, it looks for the id_token key in the body. JWTStrategy: Parses the session token into a JSON session object using the secret key. We have configured it to extract the session token from the Authentication header using ExtractJwt.fromAuthHeaderAsBearerToken().These strategies will add whatever we return using the verify callback to the req.user property of the request. Now let’s add the routes to the app in src/app/routes.ts:import { Express } from &#39;express&#39;;import authRouter from &#39;../routes/auth&#39;;import messageRouter from &#39;../routes/message&#39;;export default function(app: Express) { app.use(&#39;/api/v1/auth&#39;, authRouter); app.use(&#39;/api/v1/message&#39;, messageRouter);}The authentication router has the following implementation:import { User } from &#39;@prisma/client&#39;;import express from &#39;express&#39;;import jwt from &#39;jsonwebtoken&#39;;import passport from &#39;passport&#39;;import * as Session from &#39;../controllers/session&#39;;import { JWT_SECRET } from &#39;../values&#39;;const router = express.Router();router.post(&#39;/google&#39;, passport.authenticate(&#39;google-id-token&#39;), async (req, res) =&amp;gt; { const session = await Session.getSessionByUser(req.user as User); const credentials = jwt.sign(session, JWT_SECRET); res.json({ data: { scheme: &#39;Bearer&#39;, credentials, expiresAt: session.expiresAt, }, });});export default router;Notice that between the path and the callback we add a passport.authenticate call to tell the application to enforce the google-id-token authentication method for this endpoint. Simple as that. If the Google ID token is valid, the callback will run, otherwise it will return a 401 error code.With that done, we build a session object and convert it into a valid JSON Web Token. The consumer of this endpoint will want to know the scheme used in the Authentication header (Bearer in this case), the session token (which we send in the credentials property), and when the session expires to improve the user experience.There is a small caveat to add at this point. Notice that for each of the three possible environments we have mentioned we are going to need different Google Client IDs. But we have used a single one up until now. To work around this issue we can create one authentication strategy for each environment:passport.use(&#39;google-id-token-&amp;lt;env&amp;gt;&#39;, getGoogleTokenStrategy(GOOGLE_CLIENT_ID_&amp;lt;ENV&amp;gt;));And create one endpoint for each environment using each of these strategies.Now let’s move to the message router:import express from &#39;express&#39;;import passport from &#39;passport&#39;;import * as Message from &#39;../controllers/message&#39;;const router = express.Router();router.use(passport.authenticate(&#39;jwt&#39;, { session: false }));router.get(&#39;/&#39;, async (req, res) =&amp;gt; { const message = await Message.getRandomMessage(); res.json({ data: { message }, });});export default router;Because we expect all endpoints to be secured for this part of the application, we enforce JWT authentication at router level with the line:router.use(passport.authenticate(&#39;jwt&#39;, { session: false }));Everything after that is business logic.ResultsAt this point we have a working Login Screen that allows logging in using Google.Moreover, we have a Home Screen that shows authenticated users a random message from the server.At which point we are done with the tutorial.Sources React Native Application Express Application" }, { "title": "Counting Primes Really Fast", "url": "/posts/counting-primes-really-fast/", "categories": "", "tags": "Mathematics, Rust", "date": "2021-06-28 00:00:00 -0500", "snippet": "Suppose you are given a very large number, for example $n = 10^{12}$, and you wish to know how many numbers equal to or less than $n$ are prime. This is the prime counting function, normally denotes as $\\pi(n)$. In this article we will explore some of the methods to efficiently calculate $\\pi(n)$, and we will benchmark each in Rust.Checking every number up to $n$ for primalityLet’s write a naive implementation that we will use as a baseline. Of course, the first approach would be to test the primality of each number from $2$ to $n$. First, let’s write a naive implementation for is_prime:fn is_prime(x: usize) -&amp;gt; bool { for d in 2..x { if x % d == 0 { return false; } } return true;}Now, counting primes is as simple as iterating over the numbers that can be prime and testing each one:pub fn count(n: usize) -&amp;gt; usize { let mut result = 0; for x in 2..(n + 1) { if is_prime(x) { result += 1; } } return result;}As you might expect, this function does terribly:Begun testing the naive implementation with n = 10^5.The amount of prime numbers that are less than or equal to 10^5 is 9592.Elapsed: 1.03sFinished testing the naive implementation with n = 10^5.I couldn’t even get it to finish calculating $n = 10^6$ in under a minute.Using the Sieve of EratosthenesA better option than the naive implementation is the Sieve of Eratosthenes. There is a nice graphic on its Wikipedia entry explaining how it works, so I won’t go into detail. First of all, note that it is clearly faster than checking each prime as the operations are simpler (just addition and multiplication instead of the modulus operator). The problem with this algorithm is that you would need a flag for each number to indicate whether it is prime or not. For large enough $n$ this is untenable, as you will soon run out of memory, or the memory will take too long to allocate. Still, you could use the Sieve of Eratosthenes to rapidly compute the primes up to a given limit.A naive implementation of the sieve in Rust looks like:pub fn count(n: usize) -&amp;gt; usize { let mut result = 0; let mut flags = Vec::with_capacity(n); for _ in 0..n { flags.push(true); } flags[0] = false; for i in 0..n { if flags[i] { let x = i + 1; for m in ((i + x)..n).step_by(x) { flags[m] = false; } result += 1; } } return result;}A benchmark using $n = 10^6$ proves this function to be much faster than the naive implementation:Begun testing the sieve implementation with n = 10^6.The amount of prime numbers that are less than or equal to 10^6 is 78498.Elapsed: 6.86msFinished testing the sieve implementation with n = 10^6.Sadly, it starts to falters at around $n = 10^9$:Begun testing the sieve implementation with n = 10^9.The amount of prime numbers that are less than or equal to 10^9 is 50847534.Elapsed: 16.53sFinished testing the sieve implementation with n = 10^9.I couldn’t get it to run for $n = 10^{10}$ in under a minute.This does not imply that the sieve is useless for our purposes of calculating $\\pi(10^{12})$. In fact, we will be using it in the next few sections.Without knowing all the primes (Legendre’s Formula)To reduce the amount of memory required for our computation we will use Legendre’s Formula. It essentially allows you to count primes up to $n$, while only knowing the primes up to $\\sqrt{n}$. Now let’s proceed to prove it.Let $\\phi(n, a)$ be the amount of numbers less than or equal to $n$, which are not divisible by the first $a$ primes. Then we can calculate $\\phi(n, a)$ using the inclusion-exclusion principle:\\[\\phi(n, a) = n- \\sum_{p_i} \\lfloor \\frac{n}{p_i} \\rfloor+ \\sum_{p_i &amp;lt; p_j} \\lfloor \\frac{n}{p_i p_j} \\rfloor- \\sum_{p_i &amp;lt; p_j &amp;lt; p_k} \\lfloor \\frac{n}{p_i p_j p_k} \\rfloor+ \\dots\\]where all the $i, j, k, \\dots \\leq a$.Now, let’s prove the following fact: suppose $p \\leq n$. Then either $p$ is $1$, it has a divisor $d \\leq \\sqrt{n}$ or it is prime. If $p = 1$, we are done. If $p \\leq \\sqrt{n}$ then it has a divisor $d \\leq \\sqrt{n}$, where $d = p$. Now let $p &amp;gt; \\sqrt{n}$. We will now proceed by contradiction. Suppose $d$ is the smallest divisor of $p$ larger than $1$. Assume said divisor is larger than $\\sqrt{n}$. Clearly $p/d$ is also a divisor of $p$ and thus $p/d \\geq d &amp;gt; \\sqrt{n}$. But $p = p/d \\times d &amp;gt; \\sqrt{n} × \\sqrt{n} = n$, which is impossible. Therefore $d$ must not exceed $\\sqrt{n}$.From the previous fact we have that:\\[\\phi(n, \\pi(\\sqrt{n})) = 1 + \\pi(n) - \\pi(\\sqrt{n})\\]Which can be rearranged to find $\\pi(n)$. This is Legendre’s Formula.Now let’s write an implementation for Legendre’s Formula. First we need to find all primes up to $\\sqrt{n}$. This can be done in multiple ways, but the sieve of Eratosthenes is particularly effective and simple:fn isqrt(x: usize) -&amp;gt; usize { return (x as f64).sqrt() as usize;}// Calculate primes up to isqrt(n)fn get_primes(n: usize) -&amp;gt; Vec&amp;lt;usize&amp;gt; { let l = isqrt(n); let capacity = (1.5 * (l as f64) / (l as f64).ln()) as usize; let mut primes = Vec::with_capacity(capacity); let mut flags = Vec::with_capacity(l); for _ in 0..l { flags.push(true); } flags[0] = false; for i in 0..l { if flags[i] { let x = i + 1; for m in ((i + x)..l).step_by(x) { flags[m] = false; } primes.push(x); } } return primes;}Notice that we preallocate $1.5 \\frac{n}{log n}$ as the capacity for the primes vector. This is because $\\pi(n)$ is upper bounded by this quantity, as shown on Wikipedia.And now we need to calculate the sum. Because Legendre’s Formula is an infinite sum, we need to find a stopping point. We can get a clear cutoff point by noting that, in each sum, the smallest divisor is the product of the first $m$ primes. At some point this product is going to get larger than $n$, at which point all terms in the sum will be $0$. This logic is implemented with the function get_max_depth:fn get_max_depth(n: usize, primes: &amp;amp;Vec&amp;lt;usize&amp;gt;) -&amp;gt; usize { let mut max_depth = 0; let mut min_product = 1; while max_depth &amp;lt; primes.len() { min_product *= primes[max_depth]; if min_product &amp;gt; n { break; } max_depth += 1; } return max_depth;}Finally, we need to implement the function that calculates each of the sums. Because the number of primes that you must use in each sum is not fixed, we need to implement this function using recursion. I don’t want to go into details of how to do that, so this is what the function ends up looking like:fn calculate_sum(n: usize, primes: &amp;amp;Vec&amp;lt;usize&amp;gt;, depth: usize, level: usize, maybe_last_index: Option&amp;lt;usize&amp;gt;, product: usize) -&amp;gt; usize { if depth &amp;lt; level { return n / product; } let mut result = 0; let start_index = match maybe_last_index { Some(last_index) =&amp;gt; last_index + 1, None =&amp;gt; 0, }; let end_index = primes.len() - depth + level; for index in start_index..end_index { let next_level = level + 1; let next_last_index = Some(index); let next_product = product * primes[index]; if next_product &amp;gt; n { break; } result += calculate_sum(n, primes, depth, next_level, next_last_index, next_product); } return result;}Finally let’s write the count function, which is now trivial to implement:pub fn count(n: usize) -&amp;gt; usize { let mut result: isize = 0; let primes = get_primes(n); let max_depth = get_max_depth(n, &amp;amp;primes); // Use Legendre&#39;s Formula result += primes.len() as isize; result -= 1; for depth in 0..(max_depth + 1) { let term = calculate_sum(n, &amp;amp;primes, depth, 1, None, 1) as isize; if depth % 2 == 0 { result += term; } else { result -= term; } } return result as usize;}Benchmarks show us that Legendre’s formula is faster than the Sieve of Eratosthenes for $n = 10^9$:Begun testing the legendre implementation with n = 10^9.The amount of prime numbers that are less than or equal to 10^9 is 50847534.Elapsed: 4.29sFinished testing the legendre implementation with n = 10^9.Sadly, Legendre’s formula scales poorly. With $n = 10^{10}$ we almost reached the minute mark:Begun testing the legendre implementation with n = 10^10.The amount of prime numbers that are less than or equal to 10^10 is 455052511.Elapsed: 55.34sFinished testing the legendre implementation with n = 10^10.so it does not make sense to attempt $n = 10^{12}$.This failure to scale is due to the fact Legendre’s formula has to iterate over millions of choices of prime multiplications. Thankfully, our next approach will get rid of this problem.Meissel–Lehmer’s algorithmNow comes the interesting part of the post. In this article D. H. Lehmer introduces a very efficient formula for computing $\\pi(n)$. Let’s prove it.Let $P_k(n, a)$ be the amount of numbers less than or equal to $n$, which are not divisible by any of the first $a$ primes and which have exactly $k$ prime factors. Then:\\[\\begin{equation}\\phi(n, a) = \\sum_{k = 0}^{r - 1} P_k(n, a)\\end{equation}\\]for some finite $r$, as at some point the product of enough primes must be larger than $n$.Note that:\\[\\begin{equation}P_1(n, a) = \\pi(n) - a\\end{equation}\\]Joining $(1)$ and $(2)$ and using the fact that $P_0(n, a) = 1$ we get:\\[\\pi(n) = a - 1 + \\phi(n, a) - \\sum_{k = 2}^{r - 1} P_k(n, a)\\]And with the right choice of $a$, and an efficient method to calculate $\\phi(n, a)$ and the $P_k(n, a)$, we have an efficient formula for $\\pi(n)$.Let’s start by computing $P_2(n, a)$. Let $p_i$ be the $i$-th prime. Then:\\[P_2(n, a) = \\sum_{p_i p_j \\leq n} 1\\]\\[P_2(n, a) = \\sum_{p_a &amp;lt; p_i \\leq n / p_i} \\sum_{p_i \\leq p_j \\leq n / p_i} 1\\]\\[P_2(n, a) = \\sum_{p_a &amp;lt; p_i \\leq \\sqrt{n}} \\sum_{p_i \\leq p_j \\leq n / p_i} 1\\]\\[P_2(n, a) = \\sum_{p_a &amp;lt; p_i \\leq \\sqrt{n}} \\{ \\pi(n / p_i) - (i - 1) \\}\\]Now let $b = \\pi(\\sqrt{n})$ and $c = \\pi(\\sqrt[3]{n})$. Then we can rewrite the above as follows:\\[P_2(n, a) = \\sum_{p_a &amp;lt; p_i \\leq \\sqrt{n}} \\{ \\pi(n / p_i) - (i - 1) \\}\\]\\[P_2(n, a) = \\sum_{a &amp;lt; i \\leq b} \\{ \\pi(n / p_i) - (i - 1) \\}\\]\\[P_2(n, a) = \\sum_{a &amp;lt; i \\leq b} \\pi(n / p_i) - \\sum_{a &amp;lt; i \\leq b} (i - 1)\\]\\[\\begin{equation}P_2(n, a) = \\sum_{a &amp;lt; i \\leq b} \\pi(n / p_i) - \\frac{1}{2} (b - a) (b + a - 1) \\\\\\end{equation}\\]Now for $k = 3$, a similar procedure yields:\\[P_3(n, a) = \\sum_{p_i p_j p_l \\leq n} 1\\]\\[P_3(n, a) =\\sum_{p_a &amp;lt; p_i \\leq n / p_i^2}\\sum_{p_i \\leq p_j \\leq n / p_i p_j}\\sum_{p_j \\leq p_l \\leq n / p_i p_j} 1\\]\\[P_3(n, a) =\\sum_{p_a &amp;lt; p_i \\leq \\sqrt[3]{n}}\\sum_{p_i \\leq p_j \\leq (n / p_i)^{1/2}}\\sum_{p_j \\leq p_l \\leq n / p_i p_j} 1\\]\\[P_3(n, a) =\\sum_{a &amp;lt; i \\leq c}\\sum_{i \\leq j \\leq b_i}\\sum_{j \\leq l \\leq \\pi(n / p_i p_j)} 1\\]\\[\\begin{equation}P_3(n, a) =\\sum_{a &amp;lt; i \\leq c}\\sum_{i \\leq j \\leq b_i} \\{ \\pi(n / p_i p_j) - (j - 1) \\}\\end{equation}\\]where $b_i = \\pi((n / p_i)^{1/2})$.Now let $a = \\pi(n^{1/k})$. Choose $k$ or more primes larger than $p_a$. Then the product of those primes is larger than $n$. Therefore $P_k(n, a) = 0, P_{k + 1}(n, a) = 0, : \\dots$. Thus, we can derive several distinct formulas through the choice of $a$.If we set $a = b = \\pi(\\sqrt{n})$, we get $\\pi(n) = b - 1 + \\phi(n, b)$, which is Legendre’s formula.If we set $a = c = \\pi(\\sqrt[3]{n})$ we get $\\pi(n) = c - 1 + \\phi(n, c) - P_2(n, c)$ or equivalently from plugging $(3)$:\\[\\begin{equation}\\pi(n) = \\phi(n, c) + \\frac{1}{2} (b + c - 2) (b - c + 1) - \\sum_{c &amp;lt; i \\leq b} \\pi(n / p_i) \\end{equation}\\]which is Meissel’s formula.But the choice of $a$ we are interested in is $a = \\pi(n^{1 / 4})$. With this choice the formula expands to also include $P_3(n, a)$ as follows:\\[\\begin{equation}\\pi(n) = \\phi(n, a) + \\frac{1}{2} (b + a - 2) (b - a + 1) - \\sum_{a &amp;lt; i \\leq b} \\pi(n / p_i) - \\sum_{a &amp;lt; i \\leq c}\\sum_{i \\leq j \\leq b_i} \\{ \\pi(n / p_i p_j) - (j - 1) \\}\\end{equation}\\]which is Meissel-Lehmer’s formula. Essentially, it is a recursive version of the prime counting function, and therefore we can speed up the computation by knowing the values of $\\pi$ up to a large enough $n$ using the other algorithms. Now let’s find a way to calculate $\\phi(n, a)$ efficiently. First notice the following recursion:\\[\\begin{equation}\\phi(n, a) = \\phi(n, a - 1) - \\phi(n / p_a, a - 1)\\end{equation}\\]To prove that this is true, note that the amount of numbers that are divisible by $p_a$ but not by $p_1, p_2, \\dots, p_{a - 1}$ is given by $\\phi(n, a - 1) - \\phi(n, a)$. On the other hand, this amount can be expressed as:\\[= |\\{ \\: x \\: | \\: x = p_a x&#39; \\leq n, p_1, p_2, \\dots, p_{a - 1} \\nmid x&#39; \\: \\}|\\]\\[= |\\{ \\: x&#39; \\: | \\: x&#39; \\leq n / p_a, p_1, p_2, \\dots, p_{a - 1} \\nmid x&#39; \\: \\}|\\]\\[= \\phi(n / p_a, a - 1)\\]which proves the recursion. Before using the recursion, let’s prove a few base cases:Case 1: $\\phi(n, 0) = n$. This follows directly from the definition.Case 2: $\\phi(n, 1) = n - \\lfloor n / 2 \\rfloor$. These are the numbers that are not divisble by $2$.Case 3: $\\phi(n, 2) = n - \\lfloor n / 2 \\rfloor - \\lfloor n / 3 \\rfloor + \\lfloor n / 6 \\rfloor$. This follows from the inclusion-exclusion principle using the primes $2$ and $3$.Case 4: If $n \\leq p_a$, then $\\phi(n, a) = 1$. Clearly, all $x \\leq n$ are divisible by one of $p_1, p_2, \\dots, p_a$, except for the number $1$.With the base cases out of the way let’s write an expansion for $\\phi(n, a)$ using the recursion formula:\\[\\phi(n, a) = \\phi(n, a - 1) - \\phi(n / p_a, a - 1)\\]\\[\\phi(n, a) = \\phi(n, a - 2) - \\phi(n / p_a, a - 1) - \\phi(n / p_{a - 1}, a - 2)\\]\\[\\dots\\]\\[\\begin{equation}\\phi(n, a) = \\phi(n, 2) - \\sum_{i = 3}^a \\phi(n / p_i, i - 1)\\end{equation}\\]And with that we have all the parts we need to begin implementing Meissel-Lehmer formula.To begin implementing, first we need a list of all primes up to $b$, and a way to calculate $\\pi(n)$, for “small” values of $n$. Finding the primes can be done using the Sieve of Eratosthenes, and once we have the primes we can perform binary search on the list of primes to calculate $\\pi(n)$ up to the limit we used to calculate the primes. Our implementation looks like this:struct PrimeTable { limit: usize, primes: Vec&amp;lt;usize&amp;gt;, }impl PrimeTable { pub fn new(limit: usize) -&amp;gt; PrimeTable { let capacity = (1.5 * (limit as f64) / (limit as f64).ln()) as usize; let mut primes = Vec::with_capacity(capacity); let mut flags = Vec::with_capacity(limit); for _ in 0..limit { flags.push(true); } flags[0] = false; for i in 0..limit { if flags[i] { let x = i + 1; for m in ((i + x)..limit).step_by(x) { flags[m] = false; } primes.push(x); } } return PrimeTable { limit, primes }; } pub fn get_prime_count(&amp;amp;self, n: usize) -&amp;gt; Option&amp;lt;usize&amp;gt; { if n &amp;gt; self.limit { return None; } let mut start = 0; let mut end = self.primes.len() - 1; while end - start &amp;gt; 1 { let middle = (start + end) / 2; if self.primes[middle] &amp;lt;= n { start = middle; } else { end = middle; } } if start == end || self.primes[end] &amp;gt; n { return Some(start + 1); } else { return Some(end + 1); } } pub fn get_prime(&amp;amp;self, i: usize) -&amp;gt; usize { return self.primes[i - 1]; }}Notice that this table of primes can be reused in the implementation of the recursive version of $\\phi(n, a)$, as that function requires a list of primes. Our implementation of $phi$ follows trivially from $(8)$ and the base cases:fn phi(n: usize, a: usize, table: &amp;amp;PrimeTable) -&amp;gt; usize { if a == 0 { return n; } else if a == 1 { return n - (n / 2); } else if n &amp;lt;= table.get_prime(a) { return 1; } let mut result = n - (n / 2) - (n / 3) + (n / 6); for i in 3..(a + 1) { result -= phi(n / table.get_prime(i), i - 1, &amp;amp;table); } return result;}We need a way to calculate the integer $n$-th root of a number. For our purposes, we used the following implementation:fn integer_nth_root(n: usize, r: f64) -&amp;gt; usize { return (n as f64).powf(1.0 / r) as usize;}Because f64 is not an “exact” type, this implementation may fail for some inputs. Nevertheless, it is good enough for the scale at which we are working.Finally, Meissel-Lehmer’s implementation follows trivially from $(6)$:fn meissel_lehmer(n: usize, table: &amp;amp;PrimeTable) -&amp;gt; usize { if let Some(result) = table.get_prime_count(n) { return result; } let a = meissel_lehmer(integer_nth_root(n, 4.0), &amp;amp;table); let b = meissel_lehmer(integer_nth_root(n, 2.0), &amp;amp;table); let c = meissel_lehmer(integer_nth_root(n, 3.0), &amp;amp;table); let mut result = phi(n, a, &amp;amp;table) + ((b + a - 2) * (b - a + 1)) / 2; // Calculate P_2 for i in (a + 1)..(b + 1) { result -= meissel_lehmer(n / table.get_prime(i), &amp;amp;table); } // Calculate P_3 for i in (a + 1)..(c + 1) { let b_i = meissel_lehmer(integer_nth_root(n / table.get_prime(i), 2.0), &amp;amp;table); for j in i..(b_i + 1) { let denominator = table.get_prime(i) * table.get_prime(j); result -= meissel_lehmer(n / denominator, &amp;amp;table) - (j - 1); } } return result;}With all of these parts, our count function only needs to initialize a PrimeTable, and use meissel_lehmer on the given input.pub fn count(n: usize) -&amp;gt; usize { let limit = std::cmp::min(n, usize::pow(10, 9)); let prime_table = PrimeTable::new(limit); return meissel_lehmer(n, &amp;amp;prime_table);}Because our initial problem is calculating $\\pi(10^{12})$, we chose to calculate the prime list up to $10^9$, even though we only needed to do so up to $10^6$. This reduces the number of meissel_lehmer calls, which helps speed up the computation.Because our implementation essentially uses the sieve algorithm to calculate primes up to $n = 10^9$, we know it will take at least ~15s to finish. Let’s benchmark $n = 10^{10}$:Begun testing the meissel_lehmer implementation with n = 10^10.The amount of prime numbers that are less than or equal to 10^10 is 455052511.Elapsed: 16.09sFinished testing the meissel_lehmer implementation with n = 10^10.Notice that it does not take much to finish after the 15s mark. Let’s see if this extends all the way up to $n = 10^{12}$:Begun testing the meissel_lehmer implementation with n = 10^12.The amount of prime numbers that are less than or equal to 10^12 is 37607912018.Elapsed: 18.97sFinished testing the meissel_lehmer implementation with n = 10^12.And it does!ConclusionIn this post we’ve proven and implemented an efficient method for calculating $\\pi(n)$ for large enough $n$. Hopefully the reader has learned a new thing or two in this post!If you want to check out the code we used, head over to my Github." }, { "title": "From Zero to Data-Ready (Part 4): Building Analytics Dashboards using CubeJS and React", "url": "/posts/from-zero-to-data-ready-part-4/", "categories": "", "tags": "Data Engineering, Retail Project, From Zero to Data-Ready, CubeJS, Postgres, SQL, NodeJS, React, Recharts, Material UI", "date": "2021-06-18 00:00:00 -0500", "snippet": "In Part 3 we wrote our ETLs using Python and TDD. In this post we will focus on building an analytics platform for the company using CubeJS.What is CubeJS ?CubeJS is a tool that serves as a backend for data analytics. If one has a data warehouse and a dashboard then CubeJS can serve as an intermediary between both. All it requires is a data model definition. With just this we can build declarative visualizations on the frontend, and it will take the responsibility of building and resolving the required queries under the hood.Setting up CubeJSTo start using CubeJS we need NodeJS, and access to a compatible database, in our case Postgres. Once we have that, we can follow these instructions to setup CubeJS for our system. We can now use the following command to launch the CubeJS playground:npm run devImplementing our Data ModelLet’s implement our data model. As we discussed in a previous post, our data model has five main entities: Barcodes, Clients, Invoices, Payments, Sales. We will show how to implement invoices, the other entities will be analogous.We can start by creating a script at schema/Invoices.js. CubeJS will run this script to construct the schema definition. Inside this script we have access to the cube function which requires two arguments: the name of the table, and an object defining its properties. It looks something like this:cube(&#39;Invoices&#39;, { ...})Inside this object we can define the base query:cube(&#39;Invoices&#39;, { ... sql: ` SELECT id ,date ,salesperson_id ,salesperson_description ,(date + time) AS time ,client_id FROM invoices ` ...})This will tell CubeJS how to get this table from the database. Notice that sql can be any arbitrary query, so if your analytical entity requires JOINs with many tables, CubeJS can support that!Now let’s tell CubeJS how this table relates to others. This is important as we are often querying our objects by using filters from related tables. There are three types of relationships: hasMany and belongsTo, which define, respectively, both sides of a one-to-many relationship; hasOne which defines any side of a one-to-one relationship. With this in mind, our relationship definition will look like:cube(&#39;Invoices&#39;, { ... joins: { Clients: { relationship: &#39;belongsTo&#39;, sql: `${Invoices}.client_id = ${Clients}.id`, }, Payments: { relationship: &#39;hasMany&#39;, sql: `${Invoices}.id = ${Payments}.invoice_id AND ${Invoices}.date = ${Payments}.invoice_date`, }, Sales: { relationship: &#39;hasMany&#39;, sql: `${Invoices}.id = ${Sales}.invoice_id AND ${Invoices}.date = ${Sales}.invoice_date`, }, }, ...})where each entry in the joins object is the name of the table related to this one, the relationship attribute will define the type of relationship these tables have, and sql will tell us how to join both tables.We now have to define measures. For the case of invoices we will define only one: count. In CubeJS, count measures are very easy to implement:cube(&#39;Invoices&#39;, { ... measures: { count: { type: &#39;count&#39;, }, }, ...})We can also implement other types of measures. At the time of writing CubeJS supports the following measure types: number (arbitrary measure calculated using SQL), count, countDistinct, countDistinctApprox, sum, avg, min, max, runningTotal. An example of a complex sum measure can be found in the Sales table:cube(&#39;Sales&#39;, { ... measures: { total_revenue: { type: &#39;sum&#39;, sql: &#39;price * (1 - discount_percentage)&#39;, format: &#39;currency&#39;, }, }, ...})where sql defines the expression over which the sum occurs. Notice that we can format our measures which will improve user experience.Finally we need to define dimensions (the properties we will use to filter over the fact tables):cube(&#39;Invoices&#39;, { ... dimensions: { surrogate_key: { type: &#39;string&#39;, sql: `${Invoices}.id || &#39;-&#39; || ${Invoices}.date`, primaryKey: true, }, id: { type: &#39;number&#39;, sql: &#39;id&#39;, }, salesperson_id: { type: &#39;number&#39;, sql: &#39;salesperson_id&#39;, }, salesperson_description: { type: &#39;number&#39;, sql: &#39;salesperson_description&#39;, }, time: { type: &#39;time&#39;, sql: &#39;time&#39;, }, }, ...})With CubeJS there is a caveat when it comes to dimensions. For every table, we must define a primary key by setting the primaryKey attribute to true on one of the dimensions.In the case of invoices, the primary key is the tuple (id, date). Because CubeJS does not support compound primary keys, we need to create a dimension based on an expression instead of on a column reference and use that as a primary key. Thankfully, this is trivial to do in CubeJS as the sql attribute supports arbitrary expressions:surrogate_key: { type: &#39;string&#39;, sql: `${Invoices}.id || &#39;-&#39; || ${Invoices}.date`, primaryKey: true,},Building a Dashboard using ReactNow that we have a working data model for our server, we can run the CubeJS playground. Here we get three tabs, Build, Dashboard App, and Schema.In Schema we can look at the raw tables of the database and autogenerate the schema files we manually wrote in the last section using metadata like data types, primary keys and foreign keys. This is very useful if one has a large data warehouse already and wants to rapidly migrate to CubeJS.In Build we can construct visualizations using a GUI. This is very similar to how a business intelligence tool like Power BI work, with the advantage that we also get the frontend code for the visualization (which we can copy and paste into our dashboard), and the SQL code that this visualization will ultimately run in the data warehouse. There’s also an Add to Dashboard option, which allows us to automatically add any visualization we create here to our dashboard.Finally, Dashboard App gives us the option to build our dashboard in React, Angular or Vue, using a variety of charting libraries, and with the option to either allow or disallow the end-user from creating their own visualizations.Given that our clients have no technical expertise we will build a static dashboard, i.e. one that does not allow modifications from end-users. In our case we have a lot of expertise with React, so it makes sense to scaffold a dashboard project that uses React, Material UI and Recharts.Most of the work was already done by the scaffolding tool, but there was one aspect that it did not solve: filtering. Our clients want to have filters at the top of the page, and the dashboard underneath it. Building the filters is relatively straightforward as Material UI has all the fields and pickers one may need. The main problem is that both the header and the dashboard need to know what the filter context is, and the header needs to be able to update it.To make sure that the filter context is shared by both the header and the dashboard, we need to build a stateful React context and a React component to act as a context provider to the application. We’ll spare the details of how to achieve this, as it goes outside the scope of CubeJS, but there is a great example here.After these steps our application is working, and all that we need to do before the handover is polishing the design.ConclusionIn this series of posts we explored a possible architecture for a data infrastructure, and how to build it while following best practices. I hope that the reader can take some inspiration from this. Building data infrastructures doesn’t have to be hard!Sources From Zero to Data Ready project" }, { "title": "From Zero to Data-Ready (Part 3): Writing Test Driven ETLs using Python", "url": "/posts/from-zero-to-data-ready-part-3/", "categories": "", "tags": "Data Engineering, Retail Project, From Zero to Data-Ready, Python, Test Driven Development", "date": "2021-06-01 00:00:00 -0500", "snippet": "In Part 2 we created a data warehouse using sqitch. In this post we will focus on creating ETLs for our data. We will build our ETLs using Python under a Test Driven Development paradigm.Testing our Data PipelinesTest Driven Development is a programming paradigm that emphasizes writing tests for each of the important aspects of our application before we even write the first line of code. The reasoning is that the tests serve as a specification of how the application or its components should behave. Therefore once the tests pass we know that the application is correct.In real life, testing is never this easy though. Programs often have to make calls to external services we do not control. Thankfully, any robust testing framework should provide ways to either mock or intercept calls to the external environment. With this in mind, good tests are those that make sure that external services were called appropiately and that any internal state modification left the application in a correct state.We need to build three data pipelines: process_raw_dump: takes the database.mdb file and extracts all .csv files from it. These files are then written to a bucket in Google Cloud Storage. process_csv_dump: downloads the .csv files from the bucket, cleans the data, and uploads it to the staging schema of the data warehouse. populate_warehouse: populates the public schema of the data warehouse using the tables from the staging schema.All our pipelines will make use of Dependency Injection (DI) to simplify testing. For the uninitiated, DI is a design pattern where side effects are encapsulated into special objects. Whenever a functions needs to produce a specific side effect it asks for the object that encapsulates it as a parameter. To illustrate, assume the following function:import cacheimport serverdef is_user_tall(user_id): user = cache.get(user_id) if user is None: user = server.get(user_id) return user.height &amp;gt;= 2.0Testing this function would be hard, we would have to intercept the cache and server imports to test them. We can rewrite it instead as:def is_user_tall(cache, server, user_id): user = cache.get(user_id) if user is None: user = server.get(user_id) return user.height &amp;gt;= 2.0The behaviour should stay the same. So long as we remember to pass the cache and server objects nothing should break in our application. On the other hand, this makes testing easier, as we can effectively mock the cache and the server during testing, and write separate tests to verify the correctness of these objects.Let’s begin by establishing a directory structure for our project:src/ jobs/ ... utils/ ... ...tests/ jobs/ ... utils/ ...main.pyThe main.py file will serve as the entry point for our application. All files in src/ will have a corresponding file in tests/ testing its correctness. We will be using unittest to write our tests. This library comes with Python 3, so no setup is required. It automatically discovers tests in the source code by looking for files that start with test_. In each file it looks for functions prefixed with test_ and classes prefixed with Test. It will recognize these as the tests and run them.Each job is gonna be a function that receives a context object as input. The context object serves as a wrapper for all injected dependencies. In our cases, these dependencies manage interactions with each of the following: Google Cloud Storage Local Environment (Filesystem / Environment variables) mdbtools Data Warehouse (PostgreSQL)Let’s begin by writing a test for the Local Environment dependency. Specifically, let’s test the get_jobs function, which should read a comma-separated list of jobs to execute from the JOBS env variable, and should return a list of job names:import unittestfrom src.utils.environment import Environmentclass TestEnvironment(unittest.TestCase): environment = None def setUp(self): self.environment = Environment() def test_get_jobs_undefined(self): &quot;&quot;&quot; should return empty list if jobs env variable is None (undefined) &quot;&quot;&quot; result = self.environment.get_jobs() self.assertEqual(result, []) def test_get_jobs_defined(self): &quot;&quot;&quot; should return list of jobs from comma-separated env variable &quot;&quot;&quot; result = self.environment.get_jobs() self.assertEqual(result, [&quot;job1&quot;, &quot;job2&quot;])Notice that both of these tests cannot pass at the same time, as our tests will run with the JOBS env variable either defined or undefined. Moreover, these tests have one big issue: They have side-effects! For our small project this is not a problem, but on a larger project side effects slow down tests considerably, and can break stuff if the developer is not careful. Therefore we want our tests to lack side effects, if only for the sake of sanity.One solution is to patch calls to os.environ.get, to make sure it returns what we want on each specific test without having to look things up in the local system.import unittestfrom unittest.mock import patchfrom src.utils.environment import Environmentclass TestEnvironment(unittest.TestCase): environment = None def setUp(self): self.environment = Environment() @patch( &quot;src.utils.environment.os.environ.get&quot;, lambda key: None ) def test_get_jobs_undefined(self): &quot;&quot;&quot; should return empty list if jobs env variable is None (undefined) &quot;&quot;&quot; result = self.environment.get_jobs() self.assertEqual(result, []) @patch( &quot;src.utils.environment.os.environ.get&quot;, lambda key: &quot;job1,job2&quot; ) def test_get_jobs_defined(self): &quot;&quot;&quot; should return list of jobs from comma-separated env variable &quot;&quot;&quot; result = self.environment.get_jobs() self.assertEqual(result, [&quot;job1&quot;, &quot;job2&quot;])Now both tests can pass and there are no side effects. All the other tests for injected dependencies follow a similar pattern, so we won’t elaborate further on them.Let’s write a test for process_raw_dump. This pipeline’s only job is to export tables from a downloaded .mdb file and upload them to Google Cloud Storage. Assume we already have implemented and tested the following functions: context.environment.create_blank_directory: Creates a blank directory at the given path context.cloud.download_file: Downloads a given file from Google Cloud Storage context.mdbtools.dump_tables: Exports a whole database from an .mdb file to the local filesystem context.cloud.upload_bucket: Uploads a whole folder to Google Cloud StorageTherefore, we only need to test that these functions are being called appropiately and in the correct order. The final result looks like this:import unittestfrom unittest.mock import call, Mockfrom src.jobs.process_raw_dump import process_raw_dumpclass TestProcessRawDump(unittest.TestCase): def test_process(self): &quot;&quot;&quot; should correctly process the mdb dump in cloud storage and upload the exported csvs &quot;&quot;&quot; context_calls = [] context = Mock() context.environment.create_blank_directory.side_effect = \\ lambda dirname: context_calls.append((&quot;environment.create_blank_directory&quot;, dirname)) context.cloud.download_file.side_effect = \\ lambda bucket, filename: context_calls.append((&quot;cloud.download_file&quot;, bucket, filename)) context.mdbtools.dump_tables.side_effect = \\ lambda filename, folder_path: context_calls.append((&quot;mdbtools.dump_tables&quot;, filename, folder_path)) context.cloud.upload_bucket.side_effect = \\ lambda bucket: context_calls.append((&quot;cloud.upload_bucket&quot;, bucket)) process_raw_dump(context) expected_calls = [ (&quot;environment.create_blank_directory&quot;, &quot;mdb_store&quot;), (&quot;environment.create_blank_directory&quot;, &quot;csv_tables_store&quot;), (&quot;cloud.download_file&quot;, &quot;mdb_store&quot;, &quot;database.mdb&quot;), (&quot;mdbtools.dump_tables&quot;, &quot;mdb_store/database.mdb&quot;, &quot;csv_tables_store&quot;), (&quot;cloud.upload_bucket&quot;, &quot;csv_tables_store&quot;) ] self.assertListEqual(context_calls, expected_calls)We could’ve gone the extra mile to mock Google Cloud, and tested whether the .csv files in the mocked Google Cloud are correct at the end of the job. We finally opted not to, as this would’ve been cumbersome to write. Instead we opted for tests that leak elements of our implementation. In the end, software engineering is about tradeoffs, and in this case we choose to have leaky tests for the sake of having unit tests that are simpler to write.The tests for the other pipelines follow a similar pattern, so they are not worth discussing.Building our Data PipelinesFinally, building our data pipelines is relatively trivial. The first part is writing a framework to run our pipelines. We implement a Runner class where we register the jobs and our context object:from src.jobs.populate_warehouse import populate_warehousefrom src.jobs.process_csv_dump import process_csv_dumpfrom src.jobs.process_raw_dump import process_raw_dumpfrom src.utils.runner import Runnerfrom src.utils.context import Contextrunner = Runner()runner.register_job(&quot;process_raw_dump&quot;, process_raw_dump)runner.register_job(&quot;process_csv_dump&quot;, process_csv_dump)runner.register_job(&quot;populate_warehouse&quot;, populate_warehouse)context = Context()runner.set_context(context)runner.run_jobs()Finally, we want to be able to dynamically change the jobs we want to run. In particular, we want to be able to run commands like:JOBS=process_csv_dump,populate_warehouse python3 ./main.pyand have the framework run only thos jobs. To this effect we implement a simple run_jobs function that takes no arguments and handles the logic of reading the JOBS environment variable and triggering the right jobs. All of this is also implemented using the TDD paradigm to assure the correctness of our implementation.The final part is implementing the data pipelines. There is nothing to note about this process, so if the reader is interested in them, their implementation can be found in the source code for the project.Up NextNow that our data warehouse has been populated, we have most of the required infrastructure in place. We are only missing a Business Intelligence tool in our stack. In Part 4 we will build it using CubeJS. Stay tuned!Sources From Zero to Data Ready project" }, { "title": "From Zero to Data-Ready (Part 2): Building a SQL Data Warehouse", "url": "/posts/from-zero-to-data-ready-part-2/", "categories": "", "tags": "Data Engineering, Retail Project, From Zero to Data-Ready, Postgres, SQL, sqitch", "date": "2021-05-31 00:00:00 -0500", "snippet": "In Part 1 we defined our problem, performed some EDA and defined a data model appropiate for data analytic purposes. In this post we will explore how to build a data warehouse and ETLs using Test Driven Development. Without any more preamble, let’s start!Building a Data WarehouseA Data Warehouse is a database intended for analytical workloads. Usually, we use products like Redshift or Azure SQL Data Warehouse as our data warehousing layer as they are optimized for queries, unlike traditional RDBMS systems which are optimized for transactional operations. In our case the data is small enough that a Postgres instance can be used instead of a traditional data warehouse. The reason is that while traditional data warehouses are better at complex queries, they are also prohitively expensive, as they often run on top of distributed computing systems.For this project we’ve decided to use Google Cloud Platform, but any IaaS provider will work equally well. First, let’s spin up a Postgres instance. Ideally, you would want to spin up at least three: one for development, another for QA and the last one for production. For our purposes we will use only one, as this project is demonstrative and we will not be doing any QA or production work. Additionally, we will be using a migration tool for our schema. This implies that no extra work needs to done between deployment environments, and doing something once is good enough for this exercise.Let’s begin by installing sqitch, or migration tool:sudo apt-get install sqitch libdbd-pg-perl postgresql-client libdbd-sqlite3-perl sqlite3Once we are done with this step we can begin writing our migrations. For people unfamiliar with migrations, they are rerunnable tasks that are meant to update a database’s schema. The reason why we use migrations instead of doing stuff directly in the database is that they are repeatable and even testable.Let’s create a folder for our migrations:mkdir warehousecd warehouseAnd let’s add the following files to this folder:# warehouse/sqitch.conf[core] engine = pg# warehouse/sqitch.plan%syntax-version=1.0.0%project=warehouseNow create a few more folders that will hold our migrationsmkdir deploymkdir revertmkdir verifyIn deploy we will have our traditional migrations. In revert we shall have a migration that undoes whatever its corresponding migration in deploy does. This is to easily revert changes in production if we find out later down the line that our changes are screwing something up. Finally verify should hold scripts that independently validate that its corresponding migration in deploy ran correctly. We will not be implementing these, as they are cumbersome to write and add no value to a small project.In our database we will have two schemas: staging and public. The first schema is a temporary working space for our ETLs and the latter is where our client-facing tables will end up. The reason for this separation is that we do not want to end up with empty tables while data loads. With this separation we can load data into the staging tables and once data has loaded we can move the data into production inside a transaction. Because transactions inside the same database should run faster than loading data from an external source, the final user should not experience any downtime.The public schema already exists for Postgres instances. Therefore let’s write a migration for the staging schema. Run the following command:sqitch add staging_schema -n &quot;Add staging schema to database&quot;This creates the needed files in the deploy, revert and verify folders. Now replace the contents of these files with:# warehouse/deploy/staging_schema.sql-- Deploy warehouse:StagingSchema to pgBEGIN;CREATE SCHEMA staging;COMMIT;# warehouse/revert/staging_schema.sql-- Revert warehouse:StagingSchema from pgBEGIN;DROP SCHEMA staging;COMMIT;Let’s add another set of migrations for one of the staging tables:sqitch add staging_barcodes -n &quot;Creates the staging table for barcodes&quot;Now replace the contents of the following files:# warehouse/deploy/staging_barcodes.sql-- Deploy warehouse:StagingBarcodes to pgBEGIN;CREATE TABLE staging.barcodes ( barcode text PRIMARY KEY, reference text NULL, clothing_type_id int NULL, gender_id int NULL, silhouette_id int NULL, color_id int NULL, size_id text NULL);COMMIT;# warehouse/revert/staging_barcodes.sql-- Revert warehouse:StagingBarcodes from pgBEGIN;DROP TABLE staging.barcodes;COMMIT;To run the migrations we have created, execute the following:sqitch deploy --target db:pg://&amp;lt;user&amp;gt;:&amp;lt;password&amp;gt;@&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;dbname&amp;gt;Now you can log into your database instance and you should notice that staging.barcodes is there! The rest of the tables will follow a similar pattern, so we will not cover how to do them.Finally, we need to point out that the denormalization process we covered in Part 1 will occur in the Postgres instance, to take advantage of the database engine for the JOINs. Therefore we will end up with five tables in the public schema but have one for each fact and dimension table from the .mdb database in the staging schema.Up nextNow that we have a data warehouse ready to go, we can begin thinking about our ETLs. This will be covered in Part 3 of the series.Sources From Zero to Data Ready project" }, { "title": "From Zero to Data-Ready (Part 1): Exploratory Data Analysis", "url": "/posts/from-zero-to-data-ready-part-1/", "categories": "", "tags": "Data Engineering, Retail Project, From Zero to Data-Ready, Exploratory Data Analysis, Python, mdbtools", "date": "2021-05-30 00:00:00 -0500", "snippet": "Data science has been all the rage for a while. This is no accident: data science, done well, provides insights that give businesses the competitive edge you need nowadays to survive. What very few ever stop to think about is the infrastructure needed to perform data science at scale. Often, data is held in legacy databases or comes from disparate data sources. Data is also often dirty, i.e. it is not suitable for data science workloads. It is the Data Engineer’s job to build the infrastructure, clean the data and make it available to users.In this set of posts I’ll outline how to build a simple data architecture. I’ll try to keep things very high-level. If you are interested in the nitty-gritty, you can always check out the source code that I’ll provide at the end of each post.With all of that out of the way, let’s begin!Problem DescriptionA family-owned clothing shop wants to dive deeper into their data. They want to understand how their inventory sells, and take actions that will increase their profits. To achieve this we want to provide our clients with business intelligence dashboards where they can monitor their operation.Their POS (Point-Of-Sale) system was developed in the 90s using Microsoft Access 97. It has since been updated by several junior developers with no regards for documentation or internal consistency of the data. The end result is that we now have a database with wildly disparate data that we need to normalize for analytical uses. This is not a made-up scenario, but one that is all too common for small businesses.Note that this post will not focus on the data engineering required for large datasets. As such the kinds of problems that we will solve in these posts will be less hyper-technical than some might expect. Nevertheless, they are a great way to get into the data engineering mindset.Exploratory Data AnalysisMuch like in any data science project, data engineering projects start with understanding the data. Specifically we want to know where it is located, what shape it has, how to clean it up, and where to put it once we are done cleaning it. In our case the data comes from a Microsoft Access database. This is an embedded SQL databases, which means that all the data is stored in an .mdb file in a structured format. Thankfully there is already a toolset for *Nix environments called mdbtools which allows us to manipulate these files.First let’s install mdbtools in our system (assume we are running Ubuntu LTS):sudo apt install mdbtoolsNow, let’s assume that our file is named database.mdb. Then we can run the following command to list all tables in the database:mdb-tables -1 database.mdbAfter running this command we are left with a list of tables. In the case of the application we are working on, we see that some of these tables are: Tables used for internal application procedures that have no impact on the business model Data remnants, e.g. tables used for testing purposes that were never deleted Small dimension tables that hold no day-to-day business dataThe first two types of tables are useless for our purposes. The last type might be useful if they are related in some way to the large fact tables that hold day-to-day business data like transactions. So let’s start by looking for these fact tables.Whenever you are doing EDA it is worth asking people who are part of day-to-day operations how their business works. This allows you to build a mental model of the business, and these models usually map really well to how data is stored in a database. From interviewing employees at this company, we were able to figure out the main components of the business model: Clients, self-explanatory. Invoices, basically an identifier for each sale. Barcodes, which act as SKUs for the inventory. References, which exist to wrap information that is often shared by SKUs. For example, a green and an orange shirt might be the same in every way except in color and size. In that case they would have the same reference but different barcodes. Payments, associated with an invoice. Sales, which are each of the SKUs associated with an invoice. Inventory, self-explanatory.After skimming through the table names, we were able to map these to the elements of the business model. Now we need to export the data to begin poking through it. For this we can run the command:mdb-export database.mdb [table name] &amp;gt; [file name].csvThis will write our table into a csv file and now we can play with it! I’m going to use Jupyter Notebook to perform the EDA. The reasons are twofold. Firstly, doing EDA on Jupyter Notebook + Python using Pandas is relatively straightforward. Secondly, we are planning to write our ETLs on raw Python as our data is very small. This means that we can reuse whatever code we write here in the next step of our process.After doing our EDA we came to the following conclusions: Tables were severely denormalized, i.e. that the same data was found in two or more tables. Specifically, the table for barcodes repeated the information from the references table. Therefore, to keep a single source of truth for now on, we will only be looking at barcodes, as they fully describe an item. Data for invoices, payments and sales were split across several files. We found after talking to employees that the invoice table had grown too large and certain reporting functions in the software had stopped working as fast as they had. To solve this, they basically saved all the old data for invoices, payments and sales into backup table, and truncated the existing ones. The end result is that we may have two different invoices with the same id, and we can only tell them apart using the date. Therefore the primary key for invoices will be the tuple (id, date). Payments and sales only have an invoice id attached to them, there’s no date field in these tables. Thus at first it may seem like there is no way to map them to an invoice. But all tables were truncated the same day which allows us to map payments and sales to invoice by mapping old payments/sales to old invoices and new payments/sales to new invoices. Once we’ve mapped them to a particular invoice, we can bring over the date to the payments and sales tables to construct the invoice foreign key. Inventory is split into normal inventory and damaged inventory. We will merge these two tables into the barcodes table to obtain a simpler data model.Now that we know our fact tables, finding the dimension tables is a trivial matter of skimming through the columns in the fact tables and finding tables from the mdb-tables output that might relate to them. With all of this, we can proceed to create a data model for analytical purposes.Defining a data modelTo keep our data as simple as possible, we’ve decided to denormalize all dimensions into our fact tables. To illustrate, if we have the following structure in our database:Source DB Dimension Table id descSource DB Fact Table id date dim_idWe will transform it into the following structure:Result Fact Table id date dim_id dim_descThe final result of whatever ETL processes we end up building later on should be a set of five tables: clients, barcodes, invoices, payments and sales. These tables, even though they do not map one-to-one to the source database schema, they map one-to-one to the business model we inferred from talking to employeees. The schema is as follows:clients id name address city email work_phone cellphone neighborhood birthday home_phonebarcodes barcode reference clothing_type_id clothing_type_description gender_id gender_description silhouette_id silhouette_description color_id color_description size_id size_description quantity damagedinvoices id date time salesperson_id salesperson_description client_idpayments invoice_id invoice_date type amountsales invoice_id invoice_date barcode discount_percentage priceUp nextNow that we are done with our EDA and our data model definition, we have to build repeatable data pipelines that will keep our data in sync with whatever changes occur in the source database. We will cover this in Part 2.Sources From Zero to Data Ready project" } ]
